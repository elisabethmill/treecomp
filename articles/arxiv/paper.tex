\documentclass{article}

\usepackage{amsmath, amsfonts, array, authblk, float, graphicx, hyperref, natbib, subcaption}
\usepackage[margin=1in]{geometry}

\bibliographystyle{../apa-good}
\linespread{1.2}

\title{A comps-based approach for interpreting tree-based predictions}
\author[1]{Elisabeth Millington}
\author[2]{Scott Powers}
\affil[1]{Department of Kinesiology, Rice University}
\affil[2]{Department of Sport Management, Rice University}

\begin{document}

\maketitle

\begin{abstract}
  The abstract goes here.
\end{abstract}

\section{Introduction}

Random forests are a powerful machine learning prediction model that use the ensemble learning technique of combining multiple decision trees to mitigate overfitting. Each tree is built from a random bootstrap of the data, and the final prediction is determined by either majority tree vote (classification tasks) or averaging the tree responses (regression tasks). Random forests have the flexibility to model many different functional relationships while maintaining robustness to noise and outliers within the data \citep{breiman_random_2001}, which makes them suitable for many real-world applications.

Like other so-called ``black box'' machine learning algorithms, one drawback of the random forest is that the reasons for an individual's prediction can be difficult to interpret. Interpretability is crucial when using a machine learning algorithm because it allows the user to choose the best prediction method and it builds trust for the user in the prediction \citep{ribeiro_why_2016}. This is especially important in sports contexts because executives need to consider non-quantifiable information in their decision-making. The better a user understands how a model arrives at a prediction, the better they can determine how much it is appropriate to adjust the prediction based on subjective information from outside of the model.

Perhaps one of the most important decisions that sports executives make is selecting quarterbacks in the National Football League (NFL) draft. Quarterbacks are the backbone of offensive schemes for every NFL team and are arguably the most important position \citep{hughes_positional_2015}. Due to the nature of the position, trading for or signing a solid quarterback during free agency is uncommon, which is why drafting a quarterback with high NFL potential is crucial for team success. Since quarterbacks are highly sought after, talented college quarterbacks with perceived NFL starting potential are typically taken in the first or second round of the draft. In the NFL, this can have significant consequences because the rookie salary scale means large financial commitments to players taken at the top of the draft. In 2025, the slotted contract for No. 1 pick Cam Ward was \$48.4 million over four years, of which \$32.2 million was guaranteed as signing bonus \citep{badenhausen_nfl_2025}.

In traditional scouting for sports, draft prospect discussions and predictions are often based on comparing a current prospect with similar draft prospects in the past. Some of these past draft prospects will have gone on to successful careers, and others will have experienced less success \citep{trapasso_nfl_2025}. Scouts, managers, and coaches can assess NFL potential by comparing prospects to past players with similar collegiate careers. These historical reference players are known colloquially as ``comps'' \citep{jones_nfl_2025}, short for comparables. The simplest example of this line of reasoning is, ``This prospect reminds me most of Player X, and Player X had a successful career, so I think this player will have a successful career,'' effectively a human implementation of the $k$-nearest neighbor ($k$-NN) algorithm, with $k = 1$.

Random forests can be thought of as an adaptive $k$-NN algorithm \citep{lin_random_2006}, with the nearness of neighbors adaptively determined to minimize prediction error. Our work is moivated by this connection between random forests and the traditional lens of player comps. In this paper, we develop methods and software for extracting similarity scores from fitted random forest models in such a way that the prediction for a new data point is the weighted average of the outcomes in the training set, weighted according to these similarity scores.

The paper is organized as follows. In Section \ref{sec:data}, we describe a dataset on which we apply our methodology to evaluate NFL draft quarterback prospects, using ESPN's Total Quarterback Rating as the response variable and college statistics as the predictor variables. In Section \ref{sec:methods}, we derive similarity scores from a fitted random forest such that the model's predictions are equal to a similarity-weighted average of the training outcomes. In Section \ref{sec:results}, we interpret the predictions from the NFL quarterback draft prospect random forest model by ranking players from the training set according to their similarity scores induced by the adaptive nearest neighbors interpretation of random forests. In Section \ref{sec:software}, we describe the new R package rfcomp for calculating these similarity scores from fitted random forest objects. In Section \ref{sec:discussion}, we summarize the key findings.

\subsection{Related Work}

Previous research has been done to attempt to introduce interpretability to predictions from random forests and other tree-based machine learning algorithms. \citet{petkovic_improving_2018} describe a user-centered approach to reporting random forest results in an interpretable manner. \citet{aria_comparison_2021} compare two different techniques for interpreting the interactions between predictor variables and response variables in random forest models. In a sports context, \citet{ouyang_integration_2024} use Shapley Additive exPlanations (SHAP) values to interpret NBA game outcome predictions from an XGBoost model.

Draft models are fairly common in professional sports, as evaluating prospect talent can be key to success. One example of a draft model that uses ESPN's Total Quarterback Rating as the response variable is \citet{craig_predicting_2021}. They used logistic regression to estimate whether a quarterback would be selected in the draft and used a second regression to estimate that quarterback's NFL performance based on their final season metrics, scout grades, and player height. Their predictions are easy to interpret directly from the logistic regression model. \citet{wolfson_quarterback_2011} use their own metric of net points combined with games played to define NFL success. They estimate a logistic regression model for this metric on a combination of NFL Combine and college statistics, and they use this model to analyze how effectively NFL teams leverage data available when drafting a quarterback. Again, the logistic regression model results in predictions that are easily interpretable. \citet{berger_jumping_2021} use principal component analysis (PCA) combined with a high-order regression model to evaluate NBA prospects using their college statistics and combine data. The interpretation of the principal components is unclear, resulting in a relatively uninterpretable draft model. \citet{luo_improving_2024} propose a method for improving the way that NHL teams draft players by using both scouting reports and statistics. They use a combination of large language models and ensemble machine learning, which are both opaque in the way that they generate predictions, leading to uninterpretable predictions/reports.

While past work in sport analytics has attempted to predict draft prospect outcomes using interpretable methods (i.e. logistic regression) or machine learning methods with greater predictive accuracy, the novelty of the current work is that we achieve interpretability with a machine learning moethod. In particular, we tie our novel interpretation of random forest predictions to the concept of comps, which is well understood by scouts and executives in the sports industry. The implementation of our method is available in the treecomp R package, allowing practitioners to easily apply it to their own fitted random forests.

\section{Data}
\label{sec:data}

All data used for this study are publicly available from Sports Reference \citep{sports_reference_sports_2025}. Our dataset includes 2,114 quarterbacks who played their final season at the NCAA Division I Football Bowl Subdivision (FBS) between 2001 and 2023 (inclusive) who played more than six total games in their careers and attempted more than five passes per season. The data for each player-season include team, conference, games played, passing statistics (attempts, completions, touchdowns, interceptions), rushing statistics (attempts, yards, touchdowns), and awards (All-America designation, Heisman voting).

In addition to college statistics, we obtained each player's passing attempts and Total Quarterback Rating (QBR) \citep{burke_how_2016} from each season between 2006 and 2024 (inclusive). Only available since 2006, QBR is based on expected points added (EPA), which is the change in drive scoring expectation before and after each play. It considers each quarterback's share of their team's EPA and accounts for home-field advantage, defensive strength, and garbage time. The range of QBR is from 0 to 100, and \citet{burke_how_2016} suggests thinking of it as a percentile of performance at the individual game level. For each quarterback, we calculate their average QBR per season, and we define a player who has attempted fewer than 10 career NFL passes to have a QBR of zero. Here the scale of QBR is helpful because players who reach the NFL cannot be rated below players who never reach the NFL.

Average QBR per season is the response variable describing the success of quarterback prospects. Because of the date ranges of the data, we do not calculate this perfectly for all of the quarterbacks in our dataset. On one extremes, quarterbacks who finished their college careers in 2001 may have appeared in the NFL but only before 2006. These quarterbacks have a QBR of zero in our dataset. On the other extreme, quarterbacks who finished their college careers in 2023 and have not yet appeared in the NFL may yet do so. These quarterbacks, too, have a QBR of zero in our dataset. This is a limitation of the data.

\section{Methods}
\label{sec:methods}

\subsection{Random Forest}

Random forests are a machine learning method typically used for classification or regression \citep{breiman_random_2001}. The algorithm creates $B$ bootstrap samples of the original data and then estimates a decision within each bootstrap sample, randomly choosing at each node a subset of the features to consider for the split. The random forest's prediction is the average of the predictions of the individual treees. By fitting multiple decision trees, random forests generally achieve much lower-variance predictions than individual decision trees. The randomness introduced by the bootstrap resampling of the data and by randomly limiting eligible features at each split drives down the correlation between the trees in the forest, further reducing the variance of the fitted values.

We trained the random forest model using the R package ranger \citep{wright_ranger_2015}, which is a fast implementation of the random forest algorithm. The response variable was average QBR per season (zero for players who never appeared in the NFL). The college statistics used as predictor variables were games played per season, completed passes per season, attempted passes per season, passing yards per season, passing yards per attempt, passing touchdowns per season, passing interceptions per season, rushing attempts per season, rushing yards per season, rushing touchdowns per season, number of All-America selections, final-season Hesiman Award voting finish, final-season conference, final-season games played, final-season passing yards per attempt, and final-season passer efficiency rating.

To train the random forest model, we first implemented a random 70\%/30\% train/test split. We chose $B$ = 500 decision trees, each trained on a bootstramp sample of the data, with replacement. To determine the number of variables to consider at each node and the maximum tree size, we used the R package tuneRanger \citep{probst_tuneranger_2018}, which uses model-based optimization of out-of-bag prediction error to tune the hyperparameters. After 100 iterations of the tuning algorithm, we estimated the optimal number of variables to consider at each node to be X and the optimal minimum node size to be XX.

\subsection{Similarity Score}

\citet{lin_random_2006} previously established the relationship between $k$-NN and the random forest algorithm. In what follows, $\vec x_1, ..., \vec x_n \in \mathbb{R}^p$ represent the feature vectors in the training data, and $y_1, ..., y_n \in \mathbb{R}$ represent the outcome values in the training data. We draw $B$ bootstrap samples of the data indexed by $b \in \{1, ..., B\}$, and we fit a decision tree on each of these samples. We use $n_{b,i}$ to denote the number of times observation $i$ is included in bootstrap sample $b$, and we use $\mathcal{T}_{b,i}$ to denote the terminal node of observation $i$ in decision tree $b$. We use $|\mathcal{T}_{b,i}|$ to denote the number of training observations (with repetition) which belong to $\mathcal{T}_{b,i}$. If observation $i$ is not included in bootstrap sample $b$, then $n_{b,i} = 0$, and we define $\mathcal{T}_{b,i} = \{i\}$ and $|\mathcal{T}_{b,i}| = 1$.

From this setup, if we query the model at a new feature vector $\vec x_0 \in \mathbb{R}^p$, then the prediction of decision tree $b$ is given by
$$
  \hat{y}_b(\vec x_0) = \sum_{i = 1}^{n} w_{b,0,i} \cdot y_i,
$$
where $w_{b,0,i} = n_{b,i} \cdot \mathbb{I}\{\mathcal{T}_{b,0} = \mathcal{T}_{b,i}\} / |\mathcal{T}_{b,0}|$ is the proportion of training data in $\mathcal{T}_{b,0}$ which comes from observation $i$. The weights $w_{b,\cdot,0}$ can be thought of as defining a local neighborhood around $\vec x_0$ for tree $b$.

The prediction of the random forest for the feature vector $\vec x_0$ is simply the average of the predictions of the individual trees:
\begin{equation*}
  \hat{y}(\vec x_0) = \frac{1}{B} \sum_{b = 1}^B \left(\sum_{i = 1}^{n} w_{b,0,i} \cdot y_i\right),
\end{equation*}
which can be rewritten as
\begin{equation}
  \label{eqn:rf-as-knn}
  \hat{y}(\vec x_0) = \sum_{i = 1}^n \bar w_{0,i} \cdot y_i,
\end{equation}
where $\bar w_{0,i} = \left(\sum_{b = 1}^B w_{b,0,i}\right) / B$ is the average weight across the trees.

This formulation (\ref{eqn:rf-as-knn}) demonstrates how the random forest can be interpreted as a form of \textit{k}-NN, where the forest itself is a weighted neighborhood scheme and neighborhoods are defined by tree structure rather than distance. The training observations that share a leaf node with $\vec x_0$ in tree $b$ are considered its neighbors. The final prediction results from averaging the results across all trees, where training points that show up more frequently as neighbors receive greater weight.

We take this a step further and emphasize the interpretation of $\bar w_{0,i}$ as the similarity of $\vec x_0$ to $\vec x_i$. Explicitly, this {\it similarity score} is defined as
\begin{equation}
  \bar w_{0,i} = \frac{1}{B}\sum_{b = 1}^B
    \frac{n_{b,i}}{|\mathcal{T}_{b,0}|} \cdot \mathbb{I}\{\mathcal{T}_{b,0} = \mathcal{T}_{b,i}\}.
\end{equation}

Note that the similarity score is not symmetric. In general, $\bar w_{0,i} \ne \bar w_{i,0}$. In fact, as we have defined it, $\bar w_{i,0}$ is undefined if $\vec x_0$ is not in the training set. The similarity score $\bar w_{0,i}$ can be thought of as describing how similar is observation 0 to observation $i$, with similarity determined adaptively in a meaningful way so as to optimize random forest predictions. In contrast, defining similarity based on Euclidean distance would not put more weight on features which are more important for predicting outcomes. The similarity score $\bar w_{0,i}$ comes with the nice property that the prediction for $\vec x_0$ is recovered by calculating the weighted average of the outcome values in the training data, using $\bar w_{0,i}$ as the weights.

\section{Results}
\label{sec:results}

\subsection{Model Performance}

The model's test root mean squared error (RMSE) is 9.89, and 35.06\% of the model's variance is explained. A plot of the variable importance is below. 

\begin{figure}[H]
    \centering
%    \includegraphics[width=0.75\linewidth]{model plots/variable_importance.png}
    \caption{\textit{A histogram showing the importance of each predictor variable in the random forest model predicting career average QBR.}}
    \label{fig:importance}
\end{figure}

\begin{figure}[H]
    \centering
%    \includegraphics[width=0.75\linewidth]{model plots/3d_plot.png}
    \caption{\textit{This plot shows the predictions on the testing set as a function of college touchdowns per season and final season passer rating. The color of the point represents the QBR prediction value on a continuous scale, while the x-axis represents college yards per season, and the y-axis represents the final season passer rating.}}
\end{figure}

Figure \ref{fig:predicted-vs-actuals} shows the QBR predictions for the top 10 predicted quarterbacks in the 2025 draft. Cam Ward is predicted to have the highest QBR, which matches the common consensus that he is the best quarterback in this draft class and the fact he was taken first overall in the draft. Dillon Gabriel and Shedeur Sanders were predicted second and third respectively. While Dillon Gabriel was not previously expected to be drafted as high as Shedeur Sanders, he has had a very consistent college career and finished the highest out of any quarterback in Heisman voting this year.

\begin{figure}[H]
    \centering
%    \includegraphics[width=0.75\linewidth]{model plots/predicted_vs_actuals.png}
    \caption{\textit{Actual QBR plotted against the random forest generated QBR prediction for the testing dataset.}}
    \label{fig:predicted-vs-actuals}
\end{figure}

\input{../../tables/top_ten}

\subsection{2025 NFL Draft Comps}

Table 2 below shows the result of calculating similarity scores for the four chosen quarterback prospects. These scores are essentially the percentage of the prospect's leaf nodes that are shared with that training player. We can use these to interpret the predictions from Table 1 as the weights each of the training players has on the prospect's QBR prediction. We can see that for Cam Ward, Shedeur Sanders, and Dillon Gabriel, the vast majority of the training players with the top ten similarity scores are relatively successful starting (or formerly starting) quarterbacks. Intuitively, since they had the three highest predictions for this draft class, it would make sense that their predictions are based on players who are/were successful in the NFL (i.e. had a high career average QBR). For Jaxson Dart, we can see that only four of the ten players who have the highest similarity scores were ever starters in the NFL. This means that the other six players likely have a QBR value that is close to zero, which explains why his prediction is significantly lower than the other three prospects.

These four quarterbacks were selected to illustrate the model's interpretability and its alignment with real-world outcomes. We used Shedeur Sanders and Cam Ward as two examples because they were both initially expected to go in the first round of this year's draft. Shedeur Sanders' draft position did unexpectedly slide, and he ended up being taken in the fifth round in the draft. However, this is likely a result of his refusal to participate in the NFL Scouting Combine, coupled with a reported bad attitude in his interviews, rather than his college performance \citep{mckenna_what_nodate}. We chose Jaxson Dart because his draft stock continuously increased after the NFL Scouting Combine and his Pro Day, and he was one of the two quarterbacks taken in the first round of the NFL draft. Finally, we chose Dillon Gabriel because he finished the highest of all quarterbacks in Heisman voting this year (3rd overall). We used a model containing the full dataset (not just the training data) to calculate the similarity scores for each of these four players using the above-described methodology. It is worth noting that both Cam Ward and Shedeur Sanders started their college careers at teams that are not in the NCAA D1 FBS, as Ward played his first season at Incarnate Word and Sanders played his first two seasons at Jackson State. This means that their college statistics used in the model do not include their statistics from these schools. The table below serves as a tool for interpreting the above random forest predictions for these draft prospects. The percentages offer a quantifiable measure of similarity based on college statistics that underlie the random forest prediction output, allowing us to interpret the predictions better. We can see that Cam Ward has the highest QBR prediction, which intuitively makes sense because he is widely regarded as the best quarterback in this year's draft class. Dillon Gabriel has a slightly higher QBR prediction than Shedeur Sanders, even though Shedeur was considered a better prospect. However, Gabriel finished higher than Shedeur in Heisman voting, and Oregon had a much better record than Colorado, and Gabriel was ultimately taken before Sanders in the draft. Jaxson Dart's relatively low prediction compared to the other three makes sense because he wasn't considered a top quarterback prospect until his performance in the NFL Combine, which this model does not take into consideration. He also performed well in his interviews with teams, which is not a quantifiable measure.

\begin{table}[H]
\resizebox{\textwidth}{!}{
  \input{../../tables/side_by_side_similarity}}
  \caption{\textit{A table of the top similarity percentages for each of the four selected prospects. The similarity percentage is the percentage of nodes that the training quarterback and prospect ended in the same node based on the predictor variables.}}
\end{table}

Figure \ref{fig:prospect-plots} shows the weights of the QBRs used to calculate the predictions, with a green line representing the value of the prediction. There is also a graph that shows the similarity scores vs. QBR value. These plots were produced for each of the four 2025 prospects. The scatterplots show the training player similarity score against their QBR for each of the prospects. We can see that there is a large concentration of training players with a value of 0 for their QBR value. This results from the training dataset containing 2,114 players, with only 300 of those players recording NFL stats (meaning the rest have a value of 0 for the mean QBR). We can see that besides the high concentration of training players at zero, Cam Ward has the highest concentration of players around 55-65, which is why his prediction is the highest. On the opposite end of the spectrum, Jaxson Dart has the highest concentration of training players at around zero and the lowest concentration at relatively high QBR values. This explains why his prediction was by far the lowest of the four prospects we looked at.

\begin{figure}[H]
    \centering
%    \includegraphics[width=0.24\linewidth]{prospect plots/cw_plot1.png}
%    \includegraphics[width=0.24\linewidth]{prospect plots/ss_plot1.png}
%    \includegraphics[width=0.24\linewidth]{prospect plots/dg_plot1.png}
%    \includegraphics[width=0.24\linewidth]{prospect plots/jd_plot1.png} \\
%    \includegraphics[width=0.24\linewidth]{prospect plots/cw_plot2.png}
%    \includegraphics[width=0.24\linewidth]{prospect plots/ss_plot2.png}
%    \includegraphics[width=0.24\linewidth]{prospect plots/dg_plot2.png}
%    \includegraphics[width=0.24\linewidth]{prospect plots/jd_plot2.png}
    \caption{\textit{Top: Histogram of training players’ QBR values weighted by the proportion of shared nodes with the prospect (similarity). The green vertical line represents prospect’s random forest predicted QBR based on the similarity-weighted average. The red density curve provides a smoothed visualization of the distribution.
    Bottom: Scatter plot showing similarity scores for training players plotted against their actual QBR values. Higher points indicate greater similarity to the prospect, highlighting which historical players he shared the nodes with in the random forest model.}}
    \label{fig:prospect-plots}
\end{figure}

\section{Software}
\label{sec:software}

\section{Discussion}
\label{sec:discussion}

We have demonstrated a methodology that produces similarity scores between draft prospects and current/past NFL players based on college statistics by using the nearest-neighbors properties of the random forest algorithm. Comparisons between draft prospects and current players are important because they allow the analytical models to be presented in a more digestible way to fans and front offices. They allow front offices to make more informed decisions on draft day. Additionally, this method provides a way to interpret and understand the random forest algorithm.

This methodology is not specific to quarterbacks or even professional football. There are further applications for other position groups, as well as other sports. Similarity scores can be used when scouting running backs, wide receivers, defensive backs, linebackers, or virtually any position with well documented college statistics. Beyond football, this methodology can be adapted for basketball, baseball, or soccer and can even be used when considering bringing up players from the minor leagues.

This model's performance means that close to one third of the variance in QBR is explained by these variables, which highlights some limitations of statistical forecasting when scouting draft prospects. The variable importance rankings show the college statistics that have a tendency to translate into better NFL performance. However, these two things also highlight the importance of non-quantifiable factors – such as attitude, coachability, off-the-field issues, adaptability, and leadership.

One key strength of this approach to comps is its objectivity. It enables teams to leverage historical data to make comps, without relying solely on scouting narratives or subjective analysis. While this approach should not replace traditional scouting, it can complement the existing strategies and provide additional information to front offices. 

\bibliography{../random-forest-comps}

\end{document}
