
@misc{burke_how_2016,
	title = {How is {Total} {QBR} calculated? {We} explain our (improved) {QB} rating},
	shorttitle = {How is {Total} {QBR} calculated?},
	url = {https://www.espn.com/nfl/story/_/id/17653521/how-total-qbr-calculated-explain-our-improved-qb-rating},
	abstract = {Performance matters. Performance against strong opponents matters more, which is why we've adjusted our Total QBR metrics to account for good defenses.},
	journal = {ESPN.com},
	author = {Burke, Brian},
	month = sep,
	year = {2016},
}

@misc{sports_reference_sports_2025,
	title = {Sports {Stats}, fast, easy, and up-to-date},
	url = {https://www.sports-reference.com/},
	journal = {Sports-Reference.com},
	author = {{Sports Reference}},
	year = {2025},
}

@article{ouyang_integration_2024,
	title = {Integration of machine learning {XGBoost} and {SHAP} models for {NBA} game outcome prediction and quantitative analysis methodology},
	volume = {19},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0307478},
	doi = {10.1371/journal.pone.0307478},
	abstract = {This study investigated the application of artificial intelligence in real-time prediction of professional basketball games, identifying the variations within performance indicators that are critical in determining the outcomes of the games. Utilizing games data from the NBA seasons 2021 to 2023 as the sample, the study constructed a real-time predictive model for NBA game outcomes, integrating the machine learning XGBoost and SHAP algorithms. The model simulated the prediction of game outcomes at different time of games and effectively quantified the analysis of key factors that influenced game outcomes. The study’s results demonstrated that the XGBoost algorithm was highly effective in predicting NBA game outcomes. Key performance indicators such as field goal percentage, defensive rebounds, and turnovers were consistently related to the outcomes at all times during the game. In the first half of the game, assists were a key indicator affecting the outcome of the game. In the second half of the games, offensive rebounds and three-point shooting percentage were key indicators affecting the outcome of the games. The performance of the real-time prediction model for NBA game outcomes, which integrates machine learning XGBoost and SHAP algorithms, is found to be excellent and highly interpretable. By quantifying the factors that determine victory, it is able to provide significant decision support for coaches in arranging tactical strategies on the court. Moreover, the study provides reliable data references for sports bettors, athletes, club managers, and sponsors.},
	language = {en},
	number = {7},
	urldate = {2025-05-14},
	journal = {PLOS ONE},
	author = {Ouyang, Yan and Li, Xuewei and Zhou, Wenjia and Hong, Wei and Zheng, Weitao and Qi, Feng and Peng, Liming},
	month = jul,
	year = {2024},
	keywords = {Decision making, Decision tree learning, Decision trees, Forecasting, Machine learning, Machine learning algorithms, Sports, Support vector machines},
	pages = {e0307478},
}

@misc{badenhausen_nfl_2025,
	title = {{NFL} rookie signing bonuses jump 26\% for 2025 draft},
	url = {https://www.sportico.com/leagues/football/2025/nfl-draft-2025-rookie-contracts-signing-bonuses-record-1234849773/},
	journal = {Sportico},
	author = {Badenhausen, Kurt},
	month = apr,
	year = {2025},
}

@misc{mckenna_what_nodate,
	title = {What went wrong: {How} {Shedeur} {Sanders}' descent actually happened pre-draft {\textbar} {FOX} {Sports}},
	url = {https://www.foxsports.com/stories/nfl/inside-browns-qb-shedeur-sanders-nfl-draft-slide},
	urldate = {2025-05-07},
	author = {McKenna, Henry},
}

@article{luo_improving_2024,
	title = {Improving {NHL} draft outcome predictions using scouting reports},
	volume = {20},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1559-0410},
	url = {https://www.degruyterbrill.com/document/doi/10.1515/jqas-2024-0047/html},
	doi = {10.1515/jqas-2024-0047},
	abstract = {We leverage Large Language Models (LLMs) to extract information from scouting report texts and improve predictions of National Hockey League (NHL) draft outcomes. In parallel, we derive statistical features based on a player’s on-ice performance leading up to the draft. These two datasets are then combined using ensemble machine learning models. We find that both on-ice statistics and scouting reports have predictive value, however combining them leads to the strongest results.},
	language = {en},
	number = {4},
	urldate = {2025-04-28},
	journal = {Journal of Quantitative Analysis in Sports},
	author = {Luo, Hubert},
	month = dec,
	year = {2024},
	note = {Publisher: De Gruyter
Section: Journal of Quantitative Analysis in Sports},
	pages = {331--349},
}

@article{berger_jumping_2021,
	title = {Jumping to conclusions – an analysis of the {NBA} {Draft} {Combine} athleticism data and its influence on managerial decision-making},
	volume = {11},
	copyright = {© Emerald Publishing Limited 2021},
	issn = {2042678X},
	url = {https://www.proquest.com/docview/2578950034/abstract/49049558EB8A4D6BPQ/1},
	doi = {10.1108/SBM-11-2020-0117},
	abstract = {Purpose
The NBA Draft policy pursues the goal to provide the weakest teams with the most talented young players to close the gap to the superior competition. But it hinges on appropriate talent evaluation skills of the respective organizations. Research suggests the policy might be valid but to date unable to produce its intended results due to the “human judgement-factor”. This paper investigates specific managerial selection-behavior-influencing information to examine why decision-makers seem to fail to constantly seize the opportunities the draft presents them with.
Athleticism data produced within the NBA Draft Combine setting is strongly considered in the player evaluations and consequently informs the draft decisions of NBA managers. Curiously, research has failed to find much predictive power within the players pre-draft combine results for their post-draft performance. This paper investigates this clear disconnect, by examining the pre- and post-draft data from 2000 to 2019 using principal component and regression analysis.
Evidence for an athletic-induced decision-quality-lowering bias within the NBA Draft process was found. The analysis proves that players with better NBA Draft Combine results tend to get drafted earlier. Controlling for position, age and pre-draft performance there seems to be no proper justification based on post-draft performance for this managerial behavior. This produces systematic errors within the structure of the NBA Draft process and leads to problematic outcomes for the entire league-policy.
The paper delivers first evidence for an athleticism-induced decision-making bias regarding the NBA Draft process. Informing future selection-behavior of managers this research could improve NBA Draft decision-making quality.},
	language = {English},
	number = {5},
	urldate = {2025-04-28},
	journal = {Sport, Business and Management},
	author = {Berger, Tobias and Daumann, Frank},
	year = {2021},
	note = {Num Pages: 20
Place: Bingley, United Kingdom
Publisher: Emerald Group Publishing Limited},
	keywords = {Athlete, Athletic drafts \& trades, Athleticism, Australian football, Bias, Cognitive bias, Decision making, Decision-making, Franchises, High school basketball, League policy, NBA draft combine, Principal component analysis, Regression analysis, Research},
	pages = {515--534},
}

@article{huang_use_2021,
	title = {Use of {Machine} {Learning} and {Deep} {Learning} to {Predict} the {Outcomes} of {Major} {League} {Baseball} {Matches}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/10/4499},
	doi = {10.3390/app11104499},
	abstract = {Major League Baseball (MLB) is the highest level of professional baseball in the world and accounts for some of the most popular international sporting events. Many scholars have conducted research on predicting the outcome of MLB matches. The accuracy in predicting the results of baseball games is low. Therefore, deep learning and machine learning methods were used to build models for predicting the outcomes (win/loss) of MLB matches and investigate the differences between the models in terms of their performance. The match data of 30 teams during the 2019 MLB season with only the starting pitcher or with all pitchers in the pitcher category were collected to compare the prediction accuracy. A one-dimensional convolutional neural network (1DCNN), a traditional machine learning artificial neural network (ANN), and a support vector machine (SVM) were used to predict match outcomes with fivefold cross-validation to evaluate model performance. The highest prediction accuracies were 93.4\%, 93.91\%, and 93.90\% with the 1DCNN, ANN, SVM models, respectively, before feature selection; after feature selection, the highest accuracies obtained were 94.18\% and 94.16\% with the ANN and SVM models, respectively. The prediction results obtained with the three models were similar, and the prediction accuracies were much higher than those obtained in related studies. Moreover, a 1DCNN was used for the first time for predicting the outcome of MLB matches, and it achieved a prediction accuracy similar to that achieved by machine learning methods.},
	language = {en},
	number = {10},
	urldate = {2025-04-23},
	journal = {Applied Sciences},
	author = {Huang, Mei-Ling and Li, Yun-Zhi},
	month = jan,
	year = {2021},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial neural network, major league baseball, one-dimensional convolutional neural network, prediction model, support vector machine},
	pages = {4499},
}

@article{metulini_measuring_2023,
	title = {Measuring players’ importance in basketball using the generalized {Shapley} value},
	volume = {325},
	issn = {1572-9338},
	url = {https://doi.org/10.1007/s10479-022-04653-z},
	doi = {10.1007/s10479-022-04653-z},
	abstract = {Measuring players’ importance in team sports to help coaches and staff with the aim of winning the game is gaining relevance, mainly because of the advent of new data and advanced technologies. In this paper we evaluate each player’s importance - for the first time in basketball - as his/her average marginal contribution to the utility of an ordered subset of players, through a generalized version of the Shapley value, where the value assumed by the generalized characteristic function of the generalized coalitional game is expressed in terms of the probability a certain lineup has to win the game. In turn, such probability is estimated by applying a logistic regression model in which the response is represented by the game outcome and the Dean’s factors are used as explanatory features. Then, we estimate the generalized Shapley values of the players, with associated bootstrap confidence intervals. A novelty, allowed by explicitly considering single lineups, is represented by the possibility of forming best lineups based on players’ estimated generalized Shapley values conditional on specific constraints, such as an injury or an “a-priori” coach’s decision. A comparison of our proposed approach with industry-standard counterparts shows a strong linear relation. We show the application of our proposed method to seventeen full NBA seasons (from 2004/2005 to 2020/21). We eventually estimate generalized Shapley values for Utah Jazz players and we show how our method is allowed to be used to form best lineups.},
	language = {en},
	number = {1},
	urldate = {2025-04-23},
	journal = {Annals of Operations Research},
	author = {Metulini, Rodolfo and Gnecco, Giorgio},
	month = jun,
	year = {2023},
	keywords = {Cooperative game theory, Logistic regression, National Basketball Association, Players’ performance, Sports analytics},
	pages = {441--465},
}

@article{strumbelj_explaining_2014,
	title = {Explaining prediction models and individual predictions with feature contributions},
	volume = {41},
	copyright = {http://www.springer.com/tdm},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-013-0679-x},
	doi = {10.1007/s10115-013-0679-x},
	language = {en},
	number = {3},
	urldate = {2025-04-23},
	journal = {Knowledge and Information Systems},
	author = {Štrumbelj, Erik and Kononenko, Igor},
	month = dec,
	year = {2014},
	pages = {647--665},
}

@misc{probst_tuneranger_2018,
	title = {{tuneRanger}: {Tune} {Random} {Forest} of the 'ranger' {Package}},
	shorttitle = {{tuneRanger}},
	url = {https://CRAN.R-project.org/package=tuneRanger},
	doi = {10.32614/CRAN.package.tuneRanger},
	abstract = {Tuning random forest with one line. The package is mainly based on the packages 'ranger' and 'mlrMBO'.},
	language = {en},
	urldate = {2025-04-23},
	author = {Probst, Philipp},
	month = mar,
	year = {2018},
	note = {Institution: Comprehensive R Archive Network
Pages: 0.7},
}

@misc{wright_ranger_2015,
	title = {ranger: {A} {Fast} {Implementation} of {Random} {Forests}},
	shorttitle = {ranger},
	url = {https://CRAN.R-project.org/package=ranger},
	doi = {10.32614/CRAN.package.ranger},
	abstract = {A fast implementation of Random Forests, particularly suited for high dimensional data. Ensembles of classification, regression, survival and probability prediction trees are supported. Data from genome-wide association studies can be analyzed efficiently. In addition to data frames, datasets of class 'gwaa.data' (R package 'GenABEL') and 'dgCMatrix' (R package 'Matrix')  can be directly analyzed.},
	language = {en},
	urldate = {2025-04-23},
	author = {Wright, Marvin N.},
	month = jul,
	year = {2015},
	note = {Institution: Comprehensive R Archive Network
Pages: 0.17.0},
}

@article{petkovic_improving_2018,
	title = {Improving the explainability of {Random} {Forest} classifier – user centered approach},
	volume = {23},
	issn = {2335-6936},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728671/},
	abstract = {Machine Learning (ML) methods are now influencing major decisions about patient care, new medical methods, drug development and their use and importance are rapidly increasing in all areas. However, these ML methods are inherently complex and often difficult to understand and explain resulting in barriers to their adoption and validation. Our work (RFEX) focuses on enhancing Random Forest (RF) classifier explainability by developing easy to interpret explainability summary reports from trained RF classifiers as a way to improve the explainability for (often non-expert) users. RFEX is implemented and extensively tested on Stanford FEATURE data where RF is tasked with predicting functional sites in 3D molecules based on their electrochemical signatures (features). In developing RFEX method we apply user-centered approach driven by explainability questions and requirements collected by discussions with interested practitioners. We performed formal usability testing with 13 expert and non-expert users to verify RFEX usefulness. Analysis of RFEX explainability report and user feedback indicates its usefulness in significantly increasing explainability and user confidence in RF classification on FEATURE data. Notably, RFEX summary reports easily reveal that one needs very few (from 2–6 depending on a model) top ranked features to achieve 90\% or better of the accuracy when all 480 features are used.},
	urldate = {2025-04-23},
	journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
	author = {Petkovic, Dragutin and Altman, Russ and Wong, Mike and Vigil, Arthur},
	year = {2018},
	pmid = {29218882},
	pmcid = {PMC5728671},
	pages = {204--215},
}

@article{aria_comparison_2021,
	title = {A comparison among interpretative proposals for {Random} {Forests}},
	volume = {6},
	issn = {2666-8270},
	url = {https://www.sciencedirect.com/science/article/pii/S2666827021000475},
	doi = {10.1016/j.mlwa.2021.100094},
	abstract = {The growing success of Machine Learning (ML) is making significant improvements to predictive models, facilitating their integration in various application fields. Despite its growing success, there are some limitations and disadvantages: the most significant is the lack of interpretability that does not allow users to understand how particular decisions are made. Our study focus on one of the best performing and most used models in the Machine Learning framework, the Random Forest model. It is known as an efficient model of ensemble learning, as it ensures high predictive precision, flexibility, and immediacy; it is recognized as an intuitive and understandable approach to the construction process, but it is also considered a Black Box model due to the large number of deep decision trees produced within it. The aim of this research is twofold. We present a survey about interpretative proposal for Random Forest and then we perform a machine learning experiment providing a comparison between two methodologies, inTrees, and NodeHarvest, that represent the main approaches in the rule extraction framework. The proposed experiment compares methods performance on six real datasets covering different data characteristics: n. of observations, balanced/unbalanced response, the presence of categorical and numerical predictors. This study contributes to picture a review of the methods and tools proposed for ensemble tree interpretation, and identify, in the class of rule extraction approaches, the best proposal.},
	urldate = {2025-04-23},
	journal = {Machine Learning with Applications},
	author = {Aria, Massimo and Cuccurullo, Corrado and Gnasso, Agostino},
	month = dec,
	year = {2021},
	keywords = {Model interpretation, NodeHarvest, Random Forest, Rule extraction, inTrees},
	pages = {100094},
}

@article{hughes_positional_2015,
	title = {Positional {WAR} in the {National} {Football} {League}},
	volume = {16},
	issn = {1527-0025},
	url = {https://doi.org/10.1177/1527002515580931},
	doi = {10.1177/1527002515580931},
	abstract = {We empirically estimate positional “wins above replacement” (WAR) in the National Football League (NFL). Positional WAR measures the value of players in the NFL, by position, in terms of generating wins. WAR is a commonly used metric to evaluate individual players in professional baseball and basketball in the United States, but to the best of our knowledge, this is the first study to construct WAR measures for American football. A key challenge in constructing these measures is that individual statistics for many football players are not as well developed as in baseball and basketball. Related to this point, the productivity of individual football players, perhaps more than players in any other major sport, is highly dependent on context. We circumvent issues related to measuring productivity for individual players by constructing WAR measures at the position rather than individual level. The identifying variation that we leverage in our study is generated by arguably exogenous player injuries and suspensions. Using data from three seasons and all 32 NFL teams, we show that the most valuable positions in the NFL are quarterback, wide receiver, tight end/fullback, and offensive tackle. Perhaps our most surprising finding is that positional WAR for all positions on the defensive side of the football is zero.},
	language = {EN},
	number = {6},
	urldate = {2025-04-22},
	journal = {Journal of Sports Economics},
	author = {Hughes, Andrew and Koedel, Cory and Price, Joshua A.},
	month = aug,
	year = {2015},
	note = {Publisher: SAGE Publications},
	pages = {597--613},
}

@misc{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	doi = {10.48550/arXiv.1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2025-04-22},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1602.04938 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2025-04-22},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	keywords = {Artificial Intelligence, classification, ensemble, regression},
	pages = {5--32},
}

@article{aria_comparison_2021-1,
	title = {A comparison among interpretative proposals for {Random} {Forests}},
	volume = {6},
	issn = {2666-8270},
	url = {https://www.sciencedirect.com/science/article/pii/S2666827021000475},
	doi = {10.1016/j.mlwa.2021.100094},
	abstract = {The growing success of Machine Learning (ML) is making significant improvements to predictive models, facilitating their integration in various application fields. Despite its growing success, there are some limitations and disadvantages: the most significant is the lack of interpretability that does not allow users to understand how particular decisions are made. Our study focus on one of the best performing and most used models in the Machine Learning framework, the Random Forest model. It is known as an efficient model of ensemble learning, as it ensures high predictive precision, flexibility, and immediacy; it is recognized as an intuitive and understandable approach to the construction process, but it is also considered a Black Box model due to the large number of deep decision trees produced within it. The aim of this research is twofold. We present a survey about interpretative proposal for Random Forest and then we perform a machine learning experiment providing a comparison between two methodologies, inTrees, and NodeHarvest, that represent the main approaches in the rule extraction framework. The proposed experiment compares methods performance on six real datasets covering different data characteristics: n. of observations, balanced/unbalanced response, the presence of categorical and numerical predictors. This study contributes to picture a review of the methods and tools proposed for ensemble tree interpretation, and identify, in the class of rule extraction approaches, the best proposal.},
	urldate = {2025-04-22},
	journal = {Machine Learning with Applications},
	author = {Aria, Massimo and Cuccurullo, Corrado and Gnasso, Agostino},
	month = dec,
	year = {2021},
	keywords = {Model interpretation, NodeHarvest, Random Forest, Rule extraction, inTrees},
	pages = {100094},
}

@misc{trapasso_nfl_2025,
	title = {{NFL} {Draft} 2025: {Comparing} consensus top prospects in this year's class to former infamous busts - {CBSSports}.com},
	url = {https://www.cbssports.com/nfl/draft/news/nfl-draft-2025-comparing-consensus-top-prospects-in-this-years-class-to-former-infamous-busts/},
	urldate = {2025-04-01},
	author = {Trapasso, Chris},
	month = mar,
	year = {2025},
}

@misc{noauthor_draft_2025,
	title = {Draft experts provide {NFL} comparisons for top prospects},
	url = {https://www.giants.com/news/2025-nfl-draft-player-comps-cam-ward-shedeur-sanders-abdul-carter-travis-hunter-mason-graham},
	abstract = {Draft experts from ESPN, Pro Football Focus, and The 33rd Team have provided NFL comparisons for some of this year's top prospects.},
	language = {en-US},
	urldate = {2025-04-01},
	month = feb,
	year = {2025},
}

@misc{sikkema_2025_2025,
	title = {2025 {NFL} {Draft}: {Player} comparisons for {PFF}'s top 20 prospects},
	shorttitle = {2025 {NFL} {Draft}},
	url = {https://www.pff.com/news/draft-2025-nfl-draft-player-comparisons-for-pffs-top-20-prospects},
	abstract = {Trevor Sikkema reveals his NFL comparisons for the top-20 prospects on PFF's big board.},
	language = {en},
	urldate = {2025-04-01},
	journal = {PFF},
	author = {Sikkema, Trevor},
	month = feb,
	year = {2025},
}

@misc{muench_barnwells_2025,
	title = {Barnwell's annual all-trades {NFL} mock draft: {Proposing} 32 deals to transform {Round} 1},
	shorttitle = {Barnwell's annual all-trades {NFL} mock draft},
	url = {https://www.espn.com/nfl/draft2025/story/_/id/44427621/2025-nfl-mock-draft-all-trades-deals-32-round-1-picks-players-parsons-cousins},
	abstract = {Let's match five position groups from the 2025 NFL draft class to those of recent years, finding comps for the QBs, RBs, WRs, DTs and edge rushers.},
	language = {en},
	urldate = {2025-03-31},
	journal = {ESPN.com},
	author = {Muench, Steve},
	month = mar,
	year = {2025},
}

@misc{jones_nfl_2025,
	title = {{NFL} {Draft} experts' comparisons for {Travis} {Hunter}, career projections, and breakdown},
	url = {https://www.si.com/college/colorado/football/nfl-draft-experts-comparisons-for-travis-hunter-career-projections-and-breakdown},
	abstract = {Travis Hunter, the dynamic two-way star from Colorado, whose comps suggest a career trajectory that places him among the NFL all-time greats},
	language = {en-US},
	urldate = {2025-03-31},
	journal = {Colorado Buffaloes On SI},
	author = {Jones, Jason},
	month = feb,
	year = {2025},
	note = {Section: Football},
}

@article{wolfson_quarterback_2011,
	title = {The {Quarterback} {Prediction} {Problem}: {Forecasting} the {Performance} of {College} {Quarterbacks} {Selected} in the {NFL} {Draft}},
	volume = {7},
	issn = {1559-0410},
	shorttitle = {The {Quarterback} {Prediction} {Problem}},
	url = {https://www.degruyter.com/document/doi/10.2202/1559-0410.1302/html},
	doi = {10.2202/1559-0410.1302},
	number = {3},
	urldate = {2025-03-18},
	journal = {Journal of Quantitative Analysis in Sports},
	author = {Wolfson, Julian and Addona, Vittorio and Schmicker, Robert H},
	month = jan,
	year = {2011},
}

@article{craig_predicting_2021,
	title = {Predicting the national football league potential of college quarterbacks},
	volume = {295},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221721002034},
	doi = {10.1016/j.ejor.2021.03.013},
	language = {en},
	number = {2},
	urldate = {2025-03-18},
	journal = {European Journal of Operational Research},
	author = {Craig, J. Dean and Winchester, Niven},
	month = dec,
	year = {2021},
	pages = {733--743},
}

@article{lin_random_2006,
	title = {Random {Forests} and {Adaptive} {Nearest} {Neighbors}},
	volume = {101},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/27590719},
	abstract = {In this article we study random forests through their connection with a new framework of adaptive nearest-neighbor methods. We introduce a concept of potential nearest neighbors (k-PNNs) and show that random forests can be viewed as adaptively weighted k-PNN methods. Various aspects of random forests can be studied from this perspective. We study the effect of terminal node sizes on the prediction accuracy of random forests. We further show that random forests with adaptive splitting schemes assign weights to k-PNNs in a desirable way: for the estimation at a given target point, these random forests assign voting weights to the k-PNNs of the target point according to the local importance of different input variables. We propose a new simple splitting scheme that achieves desirable adaptivity in a straightforward fashion. This simple scheme can be combined with existing algorithms. The resulting algorithm is computationally faster and gives comparable results. Other possible aspects of random forests, such as using linear combinations in splitting, are also discussed. Simulations and real datasets are used to illustrate the results.},
	number = {474},
	urldate = {2025-03-18},
	journal = {Journal of the American Statistical Association},
	author = {Lin, Yi and Jeon, Yongho},
	year = {2006},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {578--590},
}
