
@article{yurko_nflwar_2019,
	title = {{nflWAR}: a reproducible method for offensive player evaluation in football},
	volume = {15},
	rights = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1559-0410},
	url = {https://www.degruyterbrill.com/document/doi/10.1515/jqas-2018-0010/html},
	doi = {10.1515/jqas-2018-0010},
	shorttitle = {{nflWAR}},
	abstract = {Existing methods for player evaluation in American football rely heavily on proprietary data, are often not reproducible, lag behind those of other major sports, and are not interpretable in terms of game outcomes. We present four contributions to the study of football statistics to address these issues. First, we develop the R package {nflscrapR} to provide easy access to publicly available play-by-play data from the National Football League ({NFL}). Second, we introduce a novel multinomial logistic regression approach for estimating the expected points for each play. Third, we use the expected points as input into a generalized additive model for estimating the win probability for each play. Fourth, we introduce our {nflWAR} framework, using multilevel models to isolate the contributions of individual offensive skill players in terms of their wins above replacement ({WAR}). We assess the uncertainty in {WAR} through a resampling approach specifically designed for football, and we present results for the 2017 {NFL} season. We discuss how our reproducible {WAR} framework can be extended to estimate {WAR} for players at any position if researchers have data specifying the players on the field during each play. Finally, we discuss the potential implications of this work for {NFL} teams.},
	pages = {163--183},
	number = {3},
	journaltitle = {Journal of Quantitative Analysis in Sports},
	author = {Yurko, Ronald and Ventura, Samuel and Horowitz, Maksim},
	urldate = {2025-07-24},
	date = {2019-09-01},
	langid = {english},
	keywords = {R, generalized additive models, multilevel models, multinomial logistic regression, reproducibility},
}

@online{mckenna_what_2025,
	title = {What went wrong: How Shedeur Sanders' descent actually happened pre-draft {\textbar} {FOX} Sports},
	url = {https://www.foxsports.com/stories/nfl/inside-browns-qb-shedeur-sanders-nfl-draft-slide},
	author = {{McKenna}, Henry},
	urldate = {2025-05-07},
	date = {2025-04-30},
}

@online{burke_how_2016,
	title = {How is Total {QBR} calculated? We explain our (improved) {QB} rating},
	url = {https://www.espn.com/nfl/story/_/id/17653521/how-total-qbr-calculated-explain-our-improved-qb-rating},
	shorttitle = {How is Total {QBR} calculated?},
	abstract = {Performance matters. Performance against strong opponents matters more, which is why we've adjusted our Total {QBR} metrics to account for good defenses.},
	titleaddon = {{ESPN}.com},
	author = {Burke, Brian},
	date = {2016-09-27},
}

@online{sports_reference_sports_2025,
	title = {Sports Stats, fast, easy, and up-to-date},
	url = {https://www.sports-reference.com/},
	titleaddon = {Sports-Reference.com},
	author = {{Sports Reference}},
	date = {2025},
}

@article{ouyang_integration_2024,
	title = {Integration of machine learning {XGBoost} and {SHAP} models for {NBA} game outcome prediction and quantitative analysis methodology},
	volume = {19},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0307478},
	doi = {10.1371/journal.pone.0307478},
	abstract = {This study investigated the application of artificial intelligence in real-time prediction of professional basketball games, identifying the variations within performance indicators that are critical in determining the outcomes of the games. Utilizing games data from the {NBA} seasons 2021 to 2023 as the sample, the study constructed a real-time predictive model for {NBA} game outcomes, integrating the machine learning {XGBoost} and {SHAP} algorithms. The model simulated the prediction of game outcomes at different time of games and effectively quantified the analysis of key factors that influenced game outcomes. The study’s results demonstrated that the {XGBoost} algorithm was highly effective in predicting {NBA} game outcomes. Key performance indicators such as field goal percentage, defensive rebounds, and turnovers were consistently related to the outcomes at all times during the game. In the first half of the game, assists were a key indicator affecting the outcome of the game. In the second half of the games, offensive rebounds and three-point shooting percentage were key indicators affecting the outcome of the games. The performance of the real-time prediction model for {NBA} game outcomes, which integrates machine learning {XGBoost} and {SHAP} algorithms, is found to be excellent and highly interpretable. By quantifying the factors that determine victory, it is able to provide significant decision support for coaches in arranging tactical strategies on the court. Moreover, the study provides reliable data references for sports bettors, athletes, club managers, and sponsors.},
	pages = {e0307478},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Ouyang, Yan and Li, Xuewei and Zhou, Wenjia and Hong, Wei and Zheng, Weitao and Qi, Feng and Peng, Liming},
	urldate = {2025-05-14},
	date = {2024-07-23},
	langid = {english},
	keywords = {Decision making, Decision tree learning, Decision trees, Forecasting, Machine learning, Machine learning algorithms, Sports, Support vector machines},
}

@online{badenhausen_nfl_2025,
	title = {{NFL} rookie signing bonuses jump 26\% for 2025 draft},
	url = {https://www.sportico.com/leagues/football/2025/nfl-draft-2025-rookie-contracts-signing-bonuses-record-1234849773/},
	titleaddon = {Sportico},
	author = {Badenhausen, Kurt},
	date = {2025-04-24},
}

@article{luo_improving_2024,
	title = {Improving {NHL} draft outcome predictions using scouting reports},
	volume = {20},
	rights = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	issn = {1559-0410},
	url = {https://www.degruyterbrill.com/document/doi/10.1515/jqas-2024-0047/html},
	doi = {10.1515/jqas-2024-0047},
	abstract = {We leverage Large Language Models ({LLMs}) to extract information from scouting report texts and improve predictions of National Hockey League ({NHL}) draft outcomes. In parallel, we derive statistical features based on a player’s on-ice performance leading up to the draft. These two datasets are then combined using ensemble machine learning models. We find that both on-ice statistics and scouting reports have predictive value, however combining them leads to the strongest results.},
	pages = {331--349},
	number = {4},
	journaltitle = {Journal of Quantitative Analysis in Sports},
	author = {Luo, Hubert},
	urldate = {2025-04-28},
	date = {2024-12-01},
	langid = {english},
	note = {Publisher: De Gruyter
Section: Journal of Quantitative Analysis in Sports},
}

@article{berger_jumping_2021,
	title = {Jumping to conclusions – an analysis of the {NBA} Draft Combine athleticism data and its influence on managerial decision-making},
	volume = {11},
	rights = {© Emerald Publishing Limited 2021},
	issn = {2042678X},
	url = {https://www.proquest.com/docview/2578950034/abstract/49049558EB8A4D6BPQ/1},
	doi = {10.1108/SBM-11-2020-0117},
	abstract = {Purpose
The {NBA} Draft policy pursues the goal to provide the weakest teams with the most talented young players to close the gap to the superior competition. But it hinges on appropriate talent evaluation skills of the respective organizations. Research suggests the policy might be valid but to date unable to produce its intended results due to the “human judgement-factor”. This paper investigates specific managerial selection-behavior-influencing information to examine why decision-makers seem to fail to constantly seize the opportunities the draft presents them with.
Athleticism data produced within the {NBA} Draft Combine setting is strongly considered in the player evaluations and consequently informs the draft decisions of {NBA} managers. Curiously, research has failed to find much predictive power within the players pre-draft combine results for their post-draft performance. This paper investigates this clear disconnect, by examining the pre- and post-draft data from 2000 to 2019 using principal component and regression analysis.
Evidence for an athletic-induced decision-quality-lowering bias within the {NBA} Draft process was found. The analysis proves that players with better {NBA} Draft Combine results tend to get drafted earlier. Controlling for position, age and pre-draft performance there seems to be no proper justification based on post-draft performance for this managerial behavior. This produces systematic errors within the structure of the {NBA} Draft process and leads to problematic outcomes for the entire league-policy.
The paper delivers first evidence for an athleticism-induced decision-making bias regarding the {NBA} Draft process. Informing future selection-behavior of managers this research could improve {NBA} Draft decision-making quality.},
	pages = {515--534},
	number = {5},
	journaltitle = {Sport, Business and Management},
	author = {Berger, Tobias and Daumann, Frank},
	urldate = {2025-04-28},
	date = {2021},
	note = {Num Pages: 20
Place: Bingley, United Kingdom
Publisher: Emerald Group Publishing Limited},
	keywords = {Athlete, Athletic drafts \& trades, Athleticism, Australian football, Bias, Cognitive bias, Decision making, Decision-making, Franchises, High school basketball, League policy, {NBA} draft combine, Principal component analysis, Regression analysis, Research},
}

@article{huang_use_2021,
	title = {Use of Machine Learning and Deep Learning to Predict the Outcomes of Major League Baseball Matches},
	volume = {11},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/10/4499},
	doi = {10.3390/app11104499},
	abstract = {Major League Baseball ({MLB}) is the highest level of professional baseball in the world and accounts for some of the most popular international sporting events. Many scholars have conducted research on predicting the outcome of {MLB} matches. The accuracy in predicting the results of baseball games is low. Therefore, deep learning and machine learning methods were used to build models for predicting the outcomes (win/loss) of {MLB} matches and investigate the differences between the models in terms of their performance. The match data of 30 teams during the 2019 {MLB} season with only the starting pitcher or with all pitchers in the pitcher category were collected to compare the prediction accuracy. A one-dimensional convolutional neural network (1DCNN), a traditional machine learning artificial neural network ({ANN}), and a support vector machine ({SVM}) were used to predict match outcomes with fivefold cross-validation to evaluate model performance. The highest prediction accuracies were 93.4\%, 93.91\%, and 93.90\% with the 1DCNN, {ANN}, {SVM} models, respectively, before feature selection; after feature selection, the highest accuracies obtained were 94.18\% and 94.16\% with the {ANN} and {SVM} models, respectively. The prediction results obtained with the three models were similar, and the prediction accuracies were much higher than those obtained in related studies. Moreover, a 1DCNN was used for the first time for predicting the outcome of {MLB} matches, and it achieved a prediction accuracy similar to that achieved by machine learning methods.},
	pages = {4499},
	number = {10},
	journaltitle = {Applied Sciences},
	author = {Huang, Mei-Ling and Li, Yun-Zhi},
	urldate = {2025-04-23},
	date = {2021-01},
	langid = {english},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial neural network, major league baseball, one-dimensional convolutional neural network, prediction model, support vector machine},
}

@article{metulini_measuring_2023,
	title = {Measuring players’ importance in basketball using the generalized Shapley value},
	volume = {325},
	issn = {1572-9338},
	url = {https://doi.org/10.1007/s10479-022-04653-z},
	doi = {10.1007/s10479-022-04653-z},
	abstract = {Measuring players’ importance in team sports to help coaches and staff with the aim of winning the game is gaining relevance, mainly because of the advent of new data and advanced technologies. In this paper we evaluate each player’s importance - for the first time in basketball - as his/her average marginal contribution to the utility of an ordered subset of players, through a generalized version of the Shapley value, where the value assumed by the generalized characteristic function of the generalized coalitional game is expressed in terms of the probability a certain lineup has to win the game. In turn, such probability is estimated by applying a logistic regression model in which the response is represented by the game outcome and the Dean’s factors are used as explanatory features. Then, we estimate the generalized Shapley values of the players, with associated bootstrap confidence intervals. A novelty, allowed by explicitly considering single lineups, is represented by the possibility of forming best lineups based on players’ estimated generalized Shapley values conditional on specific constraints, such as an injury or an “a-priori” coach’s decision. A comparison of our proposed approach with industry-standard counterparts shows a strong linear relation. We show the application of our proposed method to seventeen full {NBA} seasons (from 2004/2005 to 2020/21). We eventually estimate generalized Shapley values for Utah Jazz players and we show how our method is allowed to be used to form best lineups.},
	pages = {441--465},
	number = {1},
	journaltitle = {Annals of Operations Research},
	shortjournal = {Ann Oper Res},
	author = {Metulini, Rodolfo and Gnecco, Giorgio},
	urldate = {2025-04-23},
	date = {2023-06-01},
	langid = {english},
	keywords = {Cooperative game theory, Logistic regression, National Basketball Association, Players’ performance, Sports analytics},
}

@article{strumbelj_explaining_2014,
	title = {Explaining prediction models and individual predictions with feature contributions},
	volume = {41},
	rights = {http://www.springer.com/tdm},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-013-0679-x},
	doi = {10.1007/s10115-013-0679-x},
	pages = {647--665},
	number = {3},
	journaltitle = {Knowledge and Information Systems},
	shortjournal = {Knowl Inf Syst},
	author = {Štrumbelj, Erik and Kononenko, Igor},
	urldate = {2025-04-23},
	date = {2014-12},
	langid = {english},
}

@misc{probst_tuneranger_2018,
	title = {{tuneRanger}: Tune Random Forest of the 'ranger' Package},
	url = {https://CRAN.R-project.org/package=tuneRanger},
	doi = {10.32614/CRAN.package.tuneRanger},
	shorttitle = {{tuneRanger}},
	abstract = {Tuning random forest with one line. The package is mainly based on the packages 'ranger' and '{mlrMBO}'.},
	author = {Probst, Philipp},
	urldate = {2025-04-23},
	date = {2018-03-14},
	langid = {english},
	note = {Institution: Comprehensive R Archive Network
Pages: 0.7},
}

@misc{wright_ranger_2015,
	title = {ranger: A Fast Implementation of Random Forests},
	url = {https://CRAN.R-project.org/package=ranger},
	doi = {10.32614/CRAN.package.ranger},
	shorttitle = {ranger},
	abstract = {A fast implementation of Random Forests, particularly suited for high dimensional data. Ensembles of classification, regression, survival and probability prediction trees are supported. Data from genome-wide association studies can be analyzed efficiently. In addition to data frames, datasets of class 'gwaa.data' (R package '{GenABEL}') and '{dgCMatrix}' (R package 'Matrix')  can be directly analyzed.},
	author = {Wright, Marvin N.},
	urldate = {2025-04-23},
	date = {2015-07-28},
	langid = {english},
	note = {Institution: Comprehensive R Archive Network
Pages: 0.17.0},
}

@article{petkovic_improving_2018,
	title = {Improving the explainability of Random Forest classifier – user centered approach},
	volume = {23},
	issn = {2335-6936},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5728671/},
	abstract = {Machine Learning ({ML}) methods are now influencing major decisions about patient care, new medical methods, drug development and their use and importance are rapidly increasing in all areas. However, these {ML} methods are inherently complex and often difficult to understand and explain resulting in barriers to their adoption and validation. Our work ({RFEX}) focuses on enhancing Random Forest ({RF}) classifier explainability by developing easy to interpret explainability summary reports from trained {RF} classifiers as a way to improve the explainability for (often non-expert) users. {RFEX} is implemented and extensively tested on Stanford {FEATURE} data where {RF} is tasked with predicting functional sites in 3D molecules based on their electrochemical signatures (features). In developing {RFEX} method we apply user-centered approach driven by explainability questions and requirements collected by discussions with interested practitioners. We performed formal usability testing with 13 expert and non-expert users to verify {RFEX} usefulness. Analysis of {RFEX} explainability report and user feedback indicates its usefulness in significantly increasing explainability and user confidence in {RF} classification on {FEATURE} data. Notably, {RFEX} summary reports easily reveal that one needs very few (from 2–6 depending on a model) top ranked features to achieve 90\% or better of the accuracy when all 480 features are used.},
	pages = {204--215},
	journaltitle = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
	shortjournal = {Pac Symp Biocomput},
	author = {Petkovic, Dragutin and Altman, Russ and Wong, Mike and Vigil, Arthur},
	urldate = {2025-04-23},
	date = {2018},
	pmid = {29218882},
	pmcid = {PMC5728671},
}

@article{aria_comparison_2021,
	title = {A comparison among interpretative proposals for Random Forests},
	volume = {6},
	issn = {2666-8270},
	url = {https://www.sciencedirect.com/science/article/pii/S2666827021000475},
	doi = {10.1016/j.mlwa.2021.100094},
	abstract = {The growing success of Machine Learning ({ML}) is making significant improvements to predictive models, facilitating their integration in various application fields. Despite its growing success, there are some limitations and disadvantages: the most significant is the lack of interpretability that does not allow users to understand how particular decisions are made. Our study focus on one of the best performing and most used models in the Machine Learning framework, the Random Forest model. It is known as an efficient model of ensemble learning, as it ensures high predictive precision, flexibility, and immediacy; it is recognized as an intuitive and understandable approach to the construction process, but it is also considered a Black Box model due to the large number of deep decision trees produced within it. The aim of this research is twofold. We present a survey about interpretative proposal for Random Forest and then we perform a machine learning experiment providing a comparison between two methodologies, {inTrees}, and {NodeHarvest}, that represent the main approaches in the rule extraction framework. The proposed experiment compares methods performance on six real datasets covering different data characteristics: n. of observations, balanced/unbalanced response, the presence of categorical and numerical predictors. This study contributes to picture a review of the methods and tools proposed for ensemble tree interpretation, and identify, in the class of rule extraction approaches, the best proposal.},
	pages = {100094},
	journaltitle = {Machine Learning with Applications},
	shortjournal = {Machine Learning with Applications},
	author = {Aria, Massimo and Cuccurullo, Corrado and Gnasso, Agostino},
	urldate = {2025-04-23},
	date = {2021-12-15},
	keywords = {Model interpretation, {NodeHarvest}, Random Forest, Rule extraction, {inTrees}},
}

@article{hughes_positional_2015,
	title = {Positional {WAR} in the National Football League},
	volume = {16},
	issn = {1527-0025},
	url = {https://doi.org/10.1177/1527002515580931},
	doi = {10.1177/1527002515580931},
	abstract = {We empirically estimate positional “wins above replacement” ({WAR}) in the National Football League ({NFL}). Positional {WAR} measures the value of players in the {NFL}, by position, in terms of generating wins. {WAR} is a commonly used metric to evaluate individual players in professional baseball and basketball in the United States, but to the best of our knowledge, this is the first study to construct {WAR} measures for American football. A key challenge in constructing these measures is that individual statistics for many football players are not as well developed as in baseball and basketball. Related to this point, the productivity of individual football players, perhaps more than players in any other major sport, is highly dependent on context. We circumvent issues related to measuring productivity for individual players by constructing {WAR} measures at the position rather than individual level. The identifying variation that we leverage in our study is generated by arguably exogenous player injuries and suspensions. Using data from three seasons and all 32 {NFL} teams, we show that the most valuable positions in the {NFL} are quarterback, wide receiver, tight end/fullback, and offensive tackle. Perhaps our most surprising finding is that positional {WAR} for all positions on the defensive side of the football is zero.},
	pages = {597--613},
	number = {6},
	journaltitle = {Journal of Sports Economics},
	author = {Hughes, Andrew and Koedel, Cory and Price, Joshua A.},
	urldate = {2025-04-22},
	date = {2015-08-01},
	note = {Publisher: {SAGE} Publications},
}

@misc{ribeiro_why_2016,
	title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
	url = {http://arxiv.org/abs/1602.04938},
	doi = {10.48550/arXiv.1602.04938},
	shorttitle = {"Why Should I Trust You?},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	number = {{arXiv}:1602.04938},
	publisher = {{arXiv}},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	urldate = {2025-04-22},
	date = {2016-08-09},
	eprinttype = {arxiv},
	eprint = {1602.04938 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{breiman_random_2001,
	title = {Random Forests},
	volume = {45},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	pages = {5--32},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Breiman, Leo},
	urldate = {2025-04-22},
	date = {2001-10-01},
	langid = {english},
	keywords = {Artificial Intelligence, classification, ensemble, regression},
}

@article{aria_comparison_2021-1,
	title = {A comparison among interpretative proposals for Random Forests},
	volume = {6},
	issn = {2666-8270},
	url = {https://www.sciencedirect.com/science/article/pii/S2666827021000475},
	doi = {10.1016/j.mlwa.2021.100094},
	abstract = {The growing success of Machine Learning ({ML}) is making significant improvements to predictive models, facilitating their integration in various application fields. Despite its growing success, there are some limitations and disadvantages: the most significant is the lack of interpretability that does not allow users to understand how particular decisions are made. Our study focus on one of the best performing and most used models in the Machine Learning framework, the Random Forest model. It is known as an efficient model of ensemble learning, as it ensures high predictive precision, flexibility, and immediacy; it is recognized as an intuitive and understandable approach to the construction process, but it is also considered a Black Box model due to the large number of deep decision trees produced within it. The aim of this research is twofold. We present a survey about interpretative proposal for Random Forest and then we perform a machine learning experiment providing a comparison between two methodologies, {inTrees}, and {NodeHarvest}, that represent the main approaches in the rule extraction framework. The proposed experiment compares methods performance on six real datasets covering different data characteristics: n. of observations, balanced/unbalanced response, the presence of categorical and numerical predictors. This study contributes to picture a review of the methods and tools proposed for ensemble tree interpretation, and identify, in the class of rule extraction approaches, the best proposal.},
	pages = {100094},
	journaltitle = {Machine Learning with Applications},
	shortjournal = {Machine Learning with Applications},
	author = {Aria, Massimo and Cuccurullo, Corrado and Gnasso, Agostino},
	urldate = {2025-04-22},
	date = {2021-12-15},
	keywords = {Model interpretation, {NodeHarvest}, Random Forest, Rule extraction, {inTrees}},
}

@online{trapasso_nfl_2025,
	title = {{NFL} Draft 2025: Comparing consensus top prospects in this year's class to former infamous busts - {CBSSports}.com},
	url = {https://www.cbssports.com/nfl/draft/news/nfl-draft-2025-comparing-consensus-top-prospects-in-this-years-class-to-former-infamous-busts/},
	author = {Trapasso, Chris},
	urldate = {2025-04-01},
	date = {2025-03-29},
}

@online{noauthor_draft_2025,
	title = {Draft experts provide {NFL} comparisons for top prospects},
	url = {https://www.giants.com/news/2025-nfl-draft-player-comps-cam-ward-shedeur-sanders-abdul-carter-travis-hunter-mason-graham},
	abstract = {Draft experts from {ESPN}, Pro Football Focus, and The 33rd Team have provided {NFL} comparisons for some of this year's top prospects.},
	urldate = {2025-04-01},
	date = {2025-02-20},
	langid = {american},
}

@online{sikkema_2025_2025,
	title = {2025 {NFL} Draft: Player comparisons for {PFF}'s top 20 prospects},
	url = {https://www.pff.com/news/draft-2025-nfl-draft-player-comparisons-for-pffs-top-20-prospects},
	shorttitle = {2025 {NFL} Draft},
	abstract = {Trevor Sikkema reveals his {NFL} comparisons for the top-20 prospects on {PFF}'s big board.},
	titleaddon = {{PFF}},
	author = {Sikkema, Trevor},
	urldate = {2025-04-01},
	date = {2025-02-12},
	langid = {english},
}

@online{muench_barnwells_2025,
	title = {Barnwell's annual all-trades {NFL} mock draft: Proposing 32 deals to transform Round 1},
	url = {https://www.espn.com/nfl/draft2025/story/_/id/44427621/2025-nfl-mock-draft-all-trades-deals-32-round-1-picks-players-parsons-cousins},
	shorttitle = {Barnwell's annual all-trades {NFL} mock draft},
	abstract = {Let's match five position groups from the 2025 {NFL} draft class to those of recent years, finding comps for the {QBs}, {RBs}, {WRs}, {DTs} and edge rushers.},
	titleaddon = {{ESPN}.com},
	author = {Muench, Steve},
	urldate = {2025-03-31},
	date = {2025-03-17},
	langid = {english},
}

@online{jones_nfl_2025,
	title = {{NFL} Draft experts' comparisons for Travis Hunter, career projections, and breakdown},
	url = {https://www.si.com/college/colorado/football/nfl-draft-experts-comparisons-for-travis-hunter-career-projections-and-breakdown},
	abstract = {Travis Hunter, the dynamic two-way star from Colorado, whose comps suggest a career trajectory that places him among the {NFL} all-time greats},
	titleaddon = {Colorado Buffaloes On {SI}},
	author = {Jones, Jason},
	urldate = {2025-03-31},
	date = {2025-02-26},
	langid = {american},
	note = {Section: Football},
}

@article{wolfson_quarterback_2011,
	title = {The Quarterback Prediction Problem: Forecasting the Performance of College Quarterbacks Selected in the {NFL} Draft},
	volume = {7},
	issn = {1559-0410},
	url = {https://www.degruyter.com/document/doi/10.2202/1559-0410.1302/html},
	doi = {10.2202/1559-0410.1302},
	shorttitle = {The Quarterback Prediction Problem},
	number = {3},
	journaltitle = {Journal of Quantitative Analysis in Sports},
	author = {Wolfson, Julian and Addona, Vittorio and Schmicker, Robert H},
	urldate = {2025-03-18},
	date = {2011-01-19},
}

@article{craig_predicting_2021,
	title = {Predicting the national football league potential of college quarterbacks},
	volume = {295},
	issn = {03772217},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0377221721002034},
	doi = {10.1016/j.ejor.2021.03.013},
	pages = {733--743},
	number = {2},
	journaltitle = {European Journal of Operational Research},
	shortjournal = {European Journal of Operational Research},
	author = {Craig, J. Dean and Winchester, Niven},
	urldate = {2025-03-18},
	date = {2021-12},
	langid = {english},
}

@article{lin_random_2006,
	title = {Random Forests and Adaptive Nearest Neighbors},
	volume = {101},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/27590719},
	abstract = {In this article we study random forests through their connection with a new framework of adaptive nearest-neighbor methods. We introduce a concept of potential nearest neighbors (k-{PNNs}) and show that random forests can be viewed as adaptively weighted k-{PNN} methods. Various aspects of random forests can be studied from this perspective. We study the effect of terminal node sizes on the prediction accuracy of random forests. We further show that random forests with adaptive splitting schemes assign weights to k-{PNNs} in a desirable way: for the estimation at a given target point, these random forests assign voting weights to the k-{PNNs} of the target point according to the local importance of different input variables. We propose a new simple splitting scheme that achieves desirable adaptivity in a straightforward fashion. This simple scheme can be combined with existing algorithms. The resulting algorithm is computationally faster and gives comparable results. Other possible aspects of random forests, such as using linear combinations in splitting, are also discussed. Simulations and real datasets are used to illustrate the results.},
	pages = {578--590},
	number = {474},
	journaltitle = {Journal of the American Statistical Association},
	author = {Lin, Yi and Jeon, Yongho},
	urldate = {2025-03-18},
	date = {2006},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
}
